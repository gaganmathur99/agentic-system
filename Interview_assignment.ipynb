{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18fc80d6",
   "metadata": {},
   "source": [
    "## Part 3: Data Pre-processing & Feature Engineering Pipeline\n",
    "\n",
    "Author: Garvit Mathur\n",
    "\n",
    "Task: Convert raw .txt data into LiLT-ready Tensors.\n",
    "\n",
    "### Context & Challenge\n",
    "The LiLT (Language-Independent Layout Transformer) model requires two inputs: text embeddings (input_ids) and spatial embeddings (bbox). [https://huggingface.co/docs/transformers/en/model_doc/lilt#transformers.LiltForTokenClassification]\n",
    "\n",
    "However, our source data consists of flat .txt files without native Bbox coordinates. Furthermore, the LiLT tokenizer breaks words into sub-words (e.g., Agreement -> Agree, ##ment), \n",
    "\n",
    "creating a length mismatch between the original words and the token sequence.\n",
    "\n",
    "### Solution Architecture\n",
    "This notebook implements a production-grade pipeline to solve these challenges:\n",
    "\n",
    "Synthetic Spatial Extraction: A heuristic engine (SpatialFeatureExtractor) that simulates layout by calculating 2D bounding boxes based on character position.\n",
    "\n",
    "Sub-word Alignment: A tokenizer wrapper (LiLTTokenizerPipeline) that utilises the word_ids() mapping strategy to propagate the synthetic bounding boxes from parent words to all constituent sub-tokens.\n",
    "\n",
    "Type Safety: Utilisation of Python dataclasses to ensure strict schema enforcement between pipeline stages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6877ff72",
   "metadata": {},
   "source": [
    "# Install Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c11f4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers torch pydantic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2476889",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5d0f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import warnings\n",
    "from typing import List, Optional, Dict\n",
    "from transformers import AutoTokenizer, PreTrainedTokenizerFast\n",
    "from pydantic import BaseModel, ConfigDict, field_validator\n",
    "import torch\n",
    "# Suppress library warnings for cleaner output\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af76e7c",
   "metadata": {},
   "source": [
    "## Configure Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29b32eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(\"LiLTPreProcessing\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e35fe3",
   "metadata": {},
   "source": [
    "## Define Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01fc5489",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "MODEL_NAME = \"nielsr/lilt-xlm-roberta-base\"\n",
    "MAX_SEQUENCE_LENGTH = 512\n",
    "BBOX_SCALE = 1000\n",
    "PAD_TOKEN_BOX = [0, 0, 0, 0]\n",
    "\n",
    "\n",
    "# Default document coordinate system used by LiLT (0-1000)\n",
    "DOC_WIDTH = 1000\n",
    "DOC_HEIGHT = 1000\n",
    "\n",
    "# Default character and line spacing for synthetic bbox generation\n",
    "DEFAULT_CHAR_WIDTH = 10\n",
    "DEFAULT_LINE_HEIGHT = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4d1179",
   "metadata": {},
   "source": [
    "# Define Pydantic Data Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67472f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RawDocument(BaseModel):\n",
    "    doc_id: str\n",
    "    content: str\n",
    "\n",
    "class ProcessedWord(BaseModel):\n",
    "    text: str\n",
    "    bbox: List[int]  # [x0, y0, x1, y1] normalised to 0-1000\n",
    "\n",
    "    @field_validator('bbox')\n",
    "    def check_bbox_geometry(cls, bbox_value):\n",
    "        \"\"\"\n",
    "        Validates that the BBox has exactly 4 coordinates and \n",
    "        that they fall within the safe 0-1000 range.\n",
    "        \"\"\"\n",
    "        if len(bbox_value) != 4:\n",
    "            raise ValueError(\"BBox must have exactly 4 coordinates [x0, y0, x1, y1]\")\n",
    "        \n",
    "        # Check for negative values or values exceeding document bounds\n",
    "        if any(bbox < 0 for bbox in bbox_value):\n",
    "            raise ValueError(f\"BBox coordinates cannot be negative: {bbox_value}\")\n",
    "        if any(bbox > DOC_WIDTH for bbox in bbox_value):\n",
    "            raise ValueError(f\"BBox coordinates cannot exceed {DOC_WIDTH}: {bbox_value}\")\n",
    "            \n",
    "        return bbox_value\n",
    "\n",
    "class ModelInput(BaseModel):\n",
    "    \"\"\"Tensor-ready dictionary for the model.\"\"\"\n",
    "    # Configuration required to allow PyTorch Tensors inside Pydantic\n",
    "    model_config = ConfigDict(arbitrary_types_allowed=True)\n",
    "    \n",
    "    input_ids: torch.Tensor\n",
    "    attention_mask: torch.Tensor\n",
    "    bbox: torch.Tensor\n",
    "    token_type_ids: Optional[torch.Tensor] = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b035f3",
   "metadata": {},
   "source": [
    "# Pre Processing Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a204f50c",
   "metadata": {},
   "source": [
    "### Bound box Generation from txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeac04c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Spatial Feature Engineering\n",
    "class SpatialFeatureExtractor:\n",
    "    \"\"\"Generates synthetic bounding boxes for words in a plain text document.\n",
    "\n",
    "    Since legacy .txt files lack metadata, we assume a 'monospaced' font layout.\n",
    "    This class calculates the X/Y coordinates based on character count. This gives the \n",
    "    LiLT model the spatial context it needs (e.g., distinguishing a Header \n",
    "    at Y=20 from a Footer at Y=900) without needing an expensive OCR engine and converting txt file to an image.\n",
    "\n",
    "    Args:\n",
    "        char_width_scale: int spacing per character (default 10).\n",
    "        line_height_scale: int height per line (default 20).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, char_width_scale: int = DEFAULT_CHAR_WIDTH, line_height_scale: int = DEFAULT_LINE_HEIGHT):\n",
    "        self.char_width = int(char_width_scale)\n",
    "        self.line_height = int(line_height_scale)\n",
    "\n",
    "    def process_text(self, document: RawDocument) -> List[ProcessedWord]:\n",
    "        \"\"\"Convert RawDocument into a sequence of ProcessedWord with bboxes.\n",
    "\n",
    "        Empty lines advance the y cursor. Coordinates are clamped to the\n",
    "        [0, DOC_WIDTH/DOC_HEIGHT] range.\n",
    "\n",
    "        Args:\n",
    "            document: RawDocument to process.\n",
    "\n",
    "        Returns:\n",
    "            List of ProcessedWord with synthetic bounding boxes.\n",
    "        \"\"\"\n",
    "        logger.info(f\"Generating spatial features for doc: {document.doc_id}\")\n",
    "\n",
    "        words_layout: List[ProcessedWord] = []\n",
    "\n",
    "        # Split the documents into list of lines\n",
    "        lines = document.content.split(\"\\n\")\n",
    "\n",
    "        # Initialise Y cursor\n",
    "        current_y = 0\n",
    "\n",
    "        # Iterate over each line\n",
    "        for line in lines:\n",
    "            # Skip empty lines but advance the Y cursor\n",
    "            if not line.strip():\n",
    "                current_y += self.line_height\n",
    "                continue\n",
    "            \n",
    "            # Initialise X cursor for the new line\n",
    "            current_x = 0\n",
    "\n",
    "            # Split the line into words\n",
    "            words = line.split()\n",
    "\n",
    "            # Iterate over each word in the line to find the width of the word and assign bbox \n",
    "            for word in words:\n",
    "                word_w = len(word) * self.char_width\n",
    "\n",
    "                bbox = [\n",
    "                    min(max(0, current_x), DOC_WIDTH),\n",
    "                    min(max(0, current_y), DOC_HEIGHT),\n",
    "                    min(max(0, current_x + word_w), DOC_WIDTH),\n",
    "                    min(max(0, current_y + self.line_height), DOC_HEIGHT),\n",
    "                ]\n",
    "\n",
    "                words_layout.append(ProcessedWord(text=word, bbox=bbox))\n",
    "\n",
    "                # advance cursor (add a simulated space)\n",
    "                current_x += word_w + self.char_width\n",
    "\n",
    "            # next line\n",
    "            current_y += self.line_height\n",
    "\n",
    "        return words_layout\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78db7a3b",
   "metadata": {},
   "source": [
    "### Module to split words into token that the model can understand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c1b1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LiLTTokenizerPipeline:\n",
    "    \"\"\"\n",
    "    Wraps the Hugging Face AutoTokenizer to handle the critical task of \n",
    "    aligning word-level Bounding Boxes to sub-word Tokens.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = MODEL_NAME):\n",
    "        logger.info(f\"Loading tokenizer: {model_name}\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        \n",
    "        # Ensure we are using a fast tokenizer to get the 'word_ids()' method\n",
    "        if not isinstance(self.tokenizer, PreTrainedTokenizerFast):\n",
    "            raise ValueError(\"This pipeline requires a FastTokenizer (backed by Rust).\")\n",
    "\n",
    "    def normalise_bbox(self, box: List[int]) -> List[int]:\n",
    "        \"\"\"Ensure coordinates are strictly within 0-1000 range.\n",
    "\n",
    "        If not clamped, out-of-bound coordinates can lead to model errors.\n",
    "        \n",
    "        Args:\n",
    "            box: List of 4 integers [x0, y0, x1, y1]\n",
    "        \n",
    "        Returns:\n",
    "            List of 4 integers with values clamped between 0 and 1000.\n",
    "        \"\"\"\n",
    "        return [\n",
    "            max(0, min(DOC_WIDTH, x)) for x in box\n",
    "        ]\n",
    "\n",
    "    def prepare_input(self, layout_data: List[ProcessedWord]) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Converts a list of ProcessedWord objects into PyTorch tensors.\n",
    "        \n",
    "        Core Logic:\n",
    "        1. Separate text and boxes.\n",
    "        2. Tokenize text (with padding/truncation).\n",
    "        3. Map original boxes to sub-word tokens using word_ids().\n",
    "        \"\"\"\n",
    "        \n",
    "        # 1. Unpack data\n",
    "        words = [word.text for word in layout_data]\n",
    "        boxes = [self.normalise_bbox(word.bbox) for word in layout_data]\n",
    "        \n",
    "        # 2. Tokenize\n",
    "        # is_split_into_words=True is CRITICAL because we provide a list of words, not a string.\n",
    "        encoding = self.tokenizer(\n",
    "            words,\n",
    "            is_split_into_words=True,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=MAX_SEQUENCE_LENGTH,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        # 3. Align Bounding Boxes\n",
    "        # The model expects one bbox per token, but we only have one bbox per word.\n",
    "        # We must duplicate the word-bbox for every sub-token belonging to that word.\n",
    "        \n",
    "        # Transformers operate on Sub-word Tokens, but our bounding boxes are generated for Whole Words.\n",
    "        # Word: \"Agreement\" (1 Box)\n",
    "        # Tokens: ['Agree', 'ment'] (2 Tokens)\n",
    "        # If we blindly pass the list of boxes, the tensor shapes won't match the input IDs, causing the model to crash.\n",
    "        # Solution: We use the tokenizer's word_ids() method to map every sub-token back to its original word index, duplicating the bounding box for every sub-token.\n",
    "        batch_index = 0  # We are processing single doc, so batch index is 0\n",
    "        word_ids = encoding.word_ids(batch_index=batch_index)\n",
    "\n",
    "        aligned_boxes = []\n",
    "        \n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                # Special tokens (CLS, SEP, PAD) get [0, 0, 0, 0]\n",
    "                aligned_boxes.append(PAD_TOKEN_BOX)\n",
    "            else:\n",
    "                # Real tokens inherit the bbox of their parent word\n",
    "                aligned_boxes.append(boxes[word_idx])\n",
    "        \n",
    "        # Convert boxes to Tensor\n",
    "        bbox_tensor = torch.tensor([aligned_boxes])\n",
    "        \n",
    "        # 4. Construct Final Model Input\n",
    "        return {\n",
    "            \"input_ids\": encoding[\"input_ids\"],\n",
    "            \"attention_mask\": encoding[\"attention_mask\"],\n",
    "            \"bbox\": bbox_tensor\n",
    "        }\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4559ba4c",
   "metadata": {},
   "source": [
    "# Run Pipeline End-to-End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254f67c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Mock Data (Simulating a file read)\n",
    "# In production, this would be replaced by actual file I/O method that reads a txt file from either disk or cloud storage.\n",
    "def get_data() -> str:\n",
    "    '''Simulates reading a text document from file/storage.'''\n",
    "    logger.info(\"Loading raw document content.\")\n",
    "    return \"\"\"\n",
    "    AGREEMENT NO:\\t 8842-GH-12 \\n\n",
    "    DATE:       12th DECEMBER 2023 \\n\n",
    "    CUSTOMER:   GARVIT MATHUR \\n\n",
    "    AMOUNT:     $15,000 \\n\n",
    "    TERMS:      36 months \\n\n",
    "    -------------------------------------------\n",
    "    Thank you for choosing our services.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1264507e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pipeline() -> None:\n",
    "    \"\"\"\n",
    "    Orchestrates the End-to-End transformation: Raw Text -> Tensors.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Load Raw Document\n",
    "    # Simulating fetching a specific document from the batch\n",
    "    raw_content = get_data()\n",
    "    doc = RawDocument(doc_id=\"DOC-TEST-001\", content=raw_content)\n",
    "\n",
    "    try:\n",
    "        # 2. Run Spatial Extraction\n",
    "        logger.info(f\"--- Stage 1: Spatial Feature Engineering ---\")\n",
    "        extractor = SpatialFeatureExtractor()\n",
    "        layout_words = extractor.process_text(doc)\n",
    "        \n",
    "        logger.info(f\"Extracted {len(layout_words)} words with BBoxes.\")\n",
    "        logger.info(f\"Sample: {layout_words[1]}\") # Print 'NO:' and its bbox\n",
    "\n",
    "        # 3. Run Tokenization & Alignment (Transformers)\n",
    "        logger.info(f\"\\n--- Stage 2: Tokenization & Alignment ---\")\n",
    "        pipeline = LiLTTokenizerPipeline()\n",
    "        model_inputs = pipeline.prepare_input(layout_words)\n",
    "        \n",
    "        # 4. Shape Verification\n",
    "        # Critical check: Input IDs and BBox tensors must match in 2nd dimension (Seq Len)\n",
    "        logger.info(f\"Input IDs Shape: {model_inputs['input_ids'].shape}\") # Expect [1, 512]\n",
    "        logger.info(f\"BBox Shape:      {model_inputs['bbox'].shape}\")      # Expect [1, 512, 4]\n",
    "        \n",
    "        # 5. Visual Verification (The \"Senior\" Check)\n",
    "        # We decode the tokens and print them next to their assigned box to prove alignment works.\n",
    "        tokens = pipeline.tokenizer.convert_ids_to_tokens(model_inputs['input_ids'][0])\n",
    "        boxes = model_inputs['bbox'][0].tolist()\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(f\"{'TOKEN':<15} | {'ASSIGNED BBOX (x0, y0, x1, y1)':<30}\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Show first 8 tokens (skipping <s>) to demonstrate splitting logic\n",
    "        for i in range(1, 10): \n",
    "            print(f\"{tokens[i]:<15} | {boxes[i]}\")\n",
    "            \n",
    "        print(\"=\"*60 + \"\\n\")\n",
    "        logger.info(\"[SUCCESS] Pipeline Complete. Tensors ready for Inference.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Pipeline execution failed: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15a0cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the pipeline\n",
    "run_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cbde91a",
   "metadata": {},
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb9836a",
   "metadata": {},
   "source": [
    "# Code Integrity and Validation Strategy  \n",
    "\n",
    "While this notebook demonstrates the functional logic of the pre-processing pipeline, deploying this to a production environment requires strict quality assurance standards. Below are the specific techniques and libraries I would implement to guarantee code integrity and reliability in the CI/CD pipeline.\n",
    "\n",
    "1. Rigorous Unit Testing (pytest)\n",
    "    - To prevent bugs, every function requires comprehensive unit tests. I would use pytest with parameterised test cases to verify the classes and methods are tested against extreme edge cases (e.g., empty lines, extremely long words).\n",
    "\n",
    "\n",
    "2. Runtime Data Validation (pydantic)\n",
    "    - As demonstrated in the ProcessedWord class, I utilised Pydantic to enforce data validation at runtime. In production, I would extend these validators to include Cross-Field Validation (e.g., ensuring input_ids length matches bbox length exactly before tensor creation).\n",
    "\n",
    "\n",
    "3. Static Code Analysis & Type Safety\n",
    "    - Tools such as mypy (Strict Mode) and ruff should be used to ensure type safety and enforce PEP-8 compliance (formatting) across the codebase.\n",
    "\n",
    "\n",
    "4. Continuous Integration (CI/CD)\n",
    "    - I would configure Pre-commit hooks and a Cloud Build pipeline to run these checks automatically. Code would only be merged to the main branch if it passes:\n",
    "        - All Unit Tests (100% Pass Rate).\n",
    "        - Minimum code coverage requirements (e.g., pytest-cov > 90%).\n",
    "        - Static Analysis (ruff and mypy)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f5e775",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
